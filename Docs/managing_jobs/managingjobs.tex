\section{Working at OLCF (ORNL)}

\subsection{Automatic Restarting and Archiving of Data}

The submission script {\tt jaguar.run} and shell script
{\tt process.jaguar} in {\tt Util/job\_scripts/jaguar/}
are designed to allow you to run \maestro\ with minimal interaction,
while being assured that the data is archived to HPSS on the NCCS
machines.

To use the scripts, first create a directory in HPSS that has the same
name as the directory on lustre you are running in (just the directory
name, not the full path).  E.g.\ if you are running in a directory
call {\tt wdconvect\_run}, then do:
\begin{verbatim}
hsi
mkdir wdconvect_run
\end{verbatim}
(Note: if the hsi command prompts you for your password, you will need to
talk to the NCCS help desk to ask for password-less access to HPSS).

The script {\tt process.jaguar} is called from {\tt jaguar.run} and
will run in the background and continually wait until checkpoint or
plotfiles are created (actually, it always leaves the most recent one
alone, since data may still be written to it, so it waits until there
are more than 1 in the directory).

Then the script will use {\tt htar} to archive the plotfiles and checkpoints
to HPSS.  If the {\tt htar} command was successful, then the plotfiles are
copied into a {\tt plotfile/} subdirectory.  This is actually important,
since you don't want to try archiving the data a second time and
overwriting the stored copy, especially if a purge took place.

Additionally, if the {\tt ftime} executable is in your path 
({\tt ftime.f90} lives in {\tt AmrPostprocessing/F\_src/}), then the
script will create a file called {\tt ftime.out} that lists the name of the
plotfile and the corresponding simulation time.

Finally, right when the job is submitted, the script will tar up all
of the diagnostic files created by {\tt diag.f90} and {\tt ftime.out}
and archive them on HPSS.  The {\tt .tar} file is given a name that
contains the date-string to allow multiple archives to co-exist.

The {\tt jaguar.run} submission script has code in it that will look
at the most recently generated checkpoint files, make sure that they
were written out correctly (it looks to see if there is a Header file,
since that is the last thing written), and automatically set the {\tt
  --restart} flag on the run line to restart from the most recent
checkpoint file.  This allows you to job-chain a bunch of submission
and have them wait until the previous job finished and then
automatically queue up:
\begin{verbatim}
qsub -W depend=afterany:<JOB-ID>  <QSUB SCRIPT>
\end{verbatim}
where {\tt <JOB-ID>} is the id number of the job that must complete
before the new submission runs and {\tt QSUB SCRIPT} is the submission
script (e.g.\ {\tt jaguar.run}).
This way you can queue up a bunch of runs and literally leave things
alone and it will restart from the right place automatically and store
the data as it is generated.

When {\tt process.jaguar} is running, it creates a lockfile (called
{\tt process.pid}) that ensures that only one instance of the script
is running at any one time.  Sometimes if the machine crashes, the
{\tt process.pid} file will be left behind, in which case, the script
aborts.  Just delete that if you know the script is not running.

The {\tt chainsub.sh} script can be used to automatically launch a number
of jobs depending on a single, currently queued (or running) job.


\subsection{Remote \visit\ Visualization on Lens}

For large data files, visualization with \visit\ should be done with
a client running on your local machine interfacing with \visit\ running
on the remote machine.  For the {\tt lens} machine at NCCS, the proper setup
is described below.

First, on {\tt lens}, in your {\tt .bashrc}, add:
\begin{verbatim}
export PATH="/sw/analysis-x64/visit/bin/":$PATH
\end{verbatim}
(you would think that you could just add {\tt module load visit} but this
does not seem to work with \visit.

On your local machine, launch \visit.  Note: this procedure seems to
work with \visit~2.4.2, but not \visit~2.5.0 for some reason.
\begin{itemize}
\item First we setup a new host

  \begin{itemize}
  \item From the menu, select {\em options $\rightarrow$ host profiles}
  \item Create a new host by clicking on the {\em New Host} button.
  \item Enter the {\em Host nickname} as {\tt lens}
  \item Enter the {\em Remote host name} as {\tt lens.ccs.ornl.gov}
  \item Enter the {\em Path to Visit installation} as {\tt /sw/analysis-x64/visit} (not sure if this is needed)
  \item Make sure that your {\em username} is correct
  \item Check {\em Tunnel data connections through SSH}
  \end{itemize}

\item Setup the {\em Launch Profiles}
  \begin{itemize}
  \item Click on the {\em Launch Profiles} tab
  \item Click on the {\em New Profile} button
  \item Enter the {\em Profile name} as {\tt parallel}
  \item Click on the {\em Parallel} tab
  \item Check {\em Launch parallel engine}
  \item Select the {\em Parallel launch method} as {\tt qsub/mpirun}
  \item Set the {\em Partition / Pool / Queue} to {\tt computation}
  \item Change the {\em Default number of processors} to 8
  \item Set the {\em Default number of nodes} to 2
  \item Set the {\em Default Bank / Account} to {\tt AST006}
  \item Set the {\em Default Time Limit} to {\tt 00:30:00}
  \end{itemize}

\item Click on {\em Apply} and {\em Post}

\item Save your changes by selecting {\em Options $\rightarrow$ Save Settings}
\end{itemize}

To do remote visualization, select {\em File $\rightarrow$ Open}.
From the drop down list at the top, select {\tt lens}.  You will be
prompted for your password.  After that, you can navigate to the
directory on lens with the data.

To make a movie (output sequence of images):
\begin{itemize}
\item save a view in \visit\ you like as a session file (File $\rightarrow$ Save session).  
\item On lens, create a file called {\tt files.visit} which lists all
  of the files you want to visualize, one per line, with {\tt /Header}
  after the filename.  This can be done simply as:
  \begin{verbatim}
  ls -1 | grep -v processed | awk '{print $1"/Header"}' > files.visit
  \end{verbatim}
  %$
  (note: the {\tt processed} bit is for when you used the script above to 
  automatically archive the data).

\item Edit the session file, searching for the name of the plotfile you
  originally visualized, and change it to read {\tt files.visit}.  Make
  sure that the path is correct.  This may appear multiple times.

\item Relaunch \visit\ locally and restore the session (File $\rightarrow$ Restore session).  It will render the first image.  Then reopen (File $\rightarrow$ ReOpen file).  After this is done, the buttons that allow you to move through the files should become active (black).

\item Resave the session file

\item To generate the frames, you have 2 options:

  \begin{enumerate}
  \item File $\rightarrow$ Save movie.  Pick {\em New simple movie},
    then set the format to {\em PNG} and add this to the output box by
    clicking the right arrow, then in the very last screen, select:
    {Later, tell me the command to run}.

   \visit\ will pop up a box showing the command to run.  Trying to
   get the currently running session of \visit\ to generate the frames
   seems problamatic.  Note: you will probably want to edit out the
   {\tt -v x.x.x} argument in the commandline to not have it force
   to use a specific version.

  \item If the session file successfully does the remote visualization
   as desired, you can run the movie via the commandline with something like:

   \begin{verbatim}
   visit -movie -format png -geometry 1080x1080 -output subchandra_cutoff3_ \
       -start 0 -end 44 -sessionfile subchandra_radvel.session
   \end{verbatim}

  \end{enumerate}

\end{itemize}


\section{Working at NERSC}

\subsection{General Guidelines}

hopper is configured with 24 cores per node, with the cores grouped
into 4 NUMA nodes of 6 cores each.  Best performance is seen when
running with 6 threads and 4 MPI tasks per node, using the following
run line:
\begin{verbatim}
aprun -n 128 -N 4 -d 6 -S 1 -ss ./main.Linux.Cray.mpi.omp.ex inputs_3d
\end{verbatim}
Here, {\tt -n} is the number of MPI tasks, and {\tt n * d} is the
total number of cores (which should match the {\tt mppwidth} set at
the top of the submission script).

\subsection{Automatic Restarting and Archiving of Data}

The same set of submission scripts are available for hopper at NERSC in 
{\tt Util/job\_scripts/hopper/}.  They are mostly identical, with a few
minor changes for the local system configuration.  The use is the same
as described above.

\subsection{Using the {\tt AmrPostprocesing} python plotting scripts on hopper}

To build the {\tt fsnapshot.so} library, you need to do:
\begin{verbatim}
module load gcc
\end{verbatim}
{\tt f2py} is already in the path, so the library should then build without issue.
%

Then edit your {\tt .bashrc.ext} file to set the {\tt PYTHONPATH} to
the {\tt python\_plotfile} directory, e.g.:
\begin{verbatim}
export PYTHONPATH="/global/homes/z/zingale/AmrPostprocessing/F\_Src/python_plotfile"
\end{verbatim}
%
and set the {\tt PATH} to that directory,
\begin{verbatim}
export PATH="/global/homes/z/zingale/AmrPostprocessing/F_Src/python_plotfile:$PATH"
\end{verbatim}

To run the script, you need to do:
\begin{verbatim}
module load matplotlib
module load python
\end{verbatim}



\subsection{Remote visualization on hopper}

\visit\ is already configured to work with hopper.  If the host does not appear
in your local version of visit, copy the {\tt host\_nersc\_hopper.xml} file
from the {\tt .visit/allhosts/} directory under the system's \visit\ install path
to your {\tt $\mathtt{\sim}$/.visit/hosts/} directory. 
