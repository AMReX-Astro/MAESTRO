\label{ch:managingjobs}

\section{General Info}

All plotfile directories have a {\tt job\_info} file which lists as
host of parameters about the simulation, including:
\begin{itemize}
\item A descriptive name for the simulation (the {\tt job\_name} runtime
  parameter
\item The number of MPI tasks and OpenMP threads
\item The total amount of CPU-hours used up to this point
\item The data and time of the plotfile creation and the directory it was written to.
\item The build date, machine, and directory
\item The \maestro, \boxlib, and other relevant git hashes for the source
\item The directories used in the build
\item The compilers used and compilation flags
\item The number of levels and boxes for the grid
\item The properties of the species carried
\item The tolerances for the MG solves
\item Any warnings from the initialization procedure (note: these are not currently stored on restart).
\item The value of all runtime parameters (even those that were not explicitly set in the inputs file), along with an indicator showing if the default value
was overridden.
\end{itemize}
This file makes it easy to understand how to recreate the run that
produced the plotfile.


\section{Linux boxes}

\subsection{gfortran}

gfortran is probably the best-supported compiler for \maestro.  Here are some
version-specific notes:

\begin{itemize}
\item {\em gfortran 4.5.x}: gfortran 4.5 has no known issues.

\item {\em gfortran 4.6.x}: gfortran 4.6.1--4.6.3 on Fedora 
(15 and 16) have a bug that shows up in multi-processor, multilevel
runs ({\tt toy\_convect} on 4 processors, with {\tt
inputs\_2d\_ml.test} for instance).  The symptom is a crash during
initialization with the message that ``message sizes do not match
across processes in the collective'' in a {\tt parallel\_bcast} call.
This is in {\tt cluster\_f.f90}, and the issue seems to be a
multiplication error in computing the size to allocate {\tt bxs}.  The
bug is not present when you compile in debug mode, so it seems to be
related to optimization.

\item {\em gfortran 4.7.x}: gfortran 4.7.1 (prerelease) seems to have 
fixed the above issue, but now wdconvect with 2-level, effective
768$^3$ dies due to an unknown seg fault at the beginning of the run.

\item {\em gfortran 4.8.x}: This typically works well, but sometimes we get
an error allocating memory in {\tt cluster\_f.f90}.  This
is a compiler bug (affecting atleast 4.8.2 and 4.8.3):

The code runs without any problem if it is compiled with {\tt -O2
-ftree-vectorize -fno-range-check} (our default) but with {\tt
cluster\_f.f90} compiled with {\tt -O2 -ftree-vectorize
-fno-range-check -fno-tree-pre}.  The ``{\tt fno-tree-pre}'' option
turns off ``{\tt ftree-pre}'' that is turned on by ``{\tt O2}'' \\
 
GCC manual says,

{\tt -ftree-pre} \\
Perform partial redundancy elimination (PRE) on trees. This flag is enabled by default at {\tt -O2} and {\tt -O3}.



\end{itemize}




\section{Working at OLCF (ORNL)}

\subsection{Titan Compilers}

The preferred compilers on Titan are the Cray compilers.  
Cray 8.2.5 works well on titan/OLCF with MPI/OpenMP.


\subsection{Monitoring Allocations}

The {\tt showusage} and {\tt showusage -f} commands give an
overview of the usage.

\subsection{Automatic Restarting and Archiving of Data}

The submission script {\tt titan.run} and shell script
{\tt process.titan} in {\tt Util/job\_scripts/titan/}
are designed to allow you to run \maestro\ with minimal interaction,
while being assured that the data is archived to HPSS on the OLCF
machines.

To use the scripts, first create a directory in HPSS that has the same
name as the directory on lustre you are running in (just the directory
name, not the full path).  E.g.\ if you are running in a directory
call {\tt wdconvect\_run}, then do:
\begin{verbatim}
hsi
mkdir wdconvect_run
\end{verbatim}
(Note: if the hsi command prompts you for your password, you will need
to talk to the OLCF help desk to ask for password-less access to
HPSS).

The script {\tt process.titan} is called from {\tt titan.run} and will
run in the background and continually wait until checkpoint or
plotfiles are created (actually, it always leaves the most recent one
alone, since data may still be written to it, so it waits until there
are more than 1 in the directory).

Then the script will use {\tt htar} to archive the plotfiles and
checkpoints to HPSS.  If the {\tt htar} command was successful, then
the plotfiles are copied into a {\tt plotfile/} subdirectory.  This is
actually important, since you don't want to try archiving the data a
second time and overwriting the stored copy, especially if a purge
took place.

Additionally, if the {\tt ftime} executable is in your path ({\tt
ftime.f90} lives in {\tt BoxLib/Tools/Postprocessing/F\_src/}), then
the script will create a file called {\tt ftime.out} that lists the
name of the plotfile and the corresponding simulation time.

Finally, right when the job is submitted, the script will tar up all
of the diagnostic files created by {\tt diag.f90} and {\tt ftime.out}
and archive them on HPSS.  The {\tt .tar} file is given a name that
contains the date-string to allow multiple archives to co-exist.

The {\tt titan.run} submission script has code in it that will look at
the most recently generated checkpoint files, make sure that they were
written out correctly (it looks to see if there is a Header file,
since that is the last thing written), and automatically set the {\tt
--restart} flag on the run line to restart from the most recent
checkpoint file.  This allows you to job-chain a bunch of submission
and have them wait until the previous job finished and then
automatically queue up:
\begin{verbatim}
qsub -W depend=afterany:<JOB-ID>  <QSUB SCRIPT>
\end{verbatim}
where {\tt <JOB-ID>} is the id number of the job that must complete
before the new submission runs and {\tt QSUB SCRIPT} is the submission
script (e.g.\ {\tt titan.run}).  This way you can queue up a bunch of
runs and literally leave things alone and it will restart from the
right place automatically and store the data as it is generated.

When {\tt process.titan} is running, it creates a lockfile (called
{\tt process.pid}) that ensures that only one instance of the script
is running at any one time.  Sometimes if the machine crashes, the
{\tt process.pid} file will be left behind, in which case, the script
aborts.  Just delete that if you know the script is not running.

The {\tt chainsub.sh} script can be used to automatically launch a
number of jobs depending on a single, currently queued (or running)
job.


\subsection{Batch Submission of \yt\ Visualization Scripts}

\subsubsection{Rhea---preferred method}

The best way to do visualization is to use rhea, the OLCF vis machine.
You need to build \yt\ via the {\tt install\_script.sh} script {\em on
rhea}.  It also must be on a {\em Lustre filesystem}, so it is seen by
the compute node.  It is best to build it in your {\tt \$PROJWORK} directory,
since that has a longer time between purges.  Once the installation is
complete, it will tell you what script to source to define the
necessary paths.

The scripts in {\tt MAESTRO/Util/job\_scripts/rhea/} will handle the
visualization.  On rhea, the job script gives you access to the
compute node, and then you can run serial jobs or a parallel job with
{\tt mpirun}.  The {\tt process-rhea.run} script will request the
resources and run the {\tt parallel-yt-rhea} script.  {\tt
parallel-yt-rhea} will launch the visualization process (defined
via the variables at the top of the script) on all the plotfiles
found to match the prefix defined in the script.  Several serial
jobs are run together, with the script using lock files to keep track
of how many processors are in use.  When processors free, the next
file in the list is processed, and so on, until there are no more
plotfiles left to process.  If a {\tt .png} file matching the
plotfile prefix is found, then that plotfile is skipped.

Note: the line in {\tt parallel-yt-rhea} that {\tt source}s the \yt\
{\tt activate} script may need to be modified to point to the
correct \yt\ installation path.


\subsubsection{Titan}

You can also run \yt\ python scripts in the titan batch queues to do your
visualization.  You need to install \yt\ and all its dependencies
manually somewhere on a {\em Lustre filesystem}---this ensures that the
compute nodes can see it.  A good choice is the project space, since
that has a longer purge window.

Once \yt\ is install by the {\tt install\_script.sh} script, it will
tell you what script to source to define the necessary paths.

The scripts {\tt vis-titan.run} and {\tt parallel-yt-new} in {\tt
MAESTRO/Util/job\_scripts/titan/} will manage the \yt jobs by calling a
python script for each file that matches a pattern.  Make sure you set
the proper path for the \yt\ {\tt activate} script in {\tt
parallel-yt-new}---this ensures that the paths to the various libraries
are set on the compute nodes.

Note that the actual visualization command itself is launched by
{\tt parallel-yt-new} using the {\tt aprun} command.  This is the
only way to get a job to run on the compute nodes on titan.  But
{\tt aprun} can only launch a single job at a time, so this means
we cannot easily do (trivally) parallel visualization on a node.  For
this reason, running on rhea is preferred.


\subsection{Remote \visit\ Visualization on Lens}

{\em Note: this information may be out-of-date.  It is recommended that
 \yt\ be used instead.}

For large data files, visualization with \visit\ should be done with
a client running on your local machine interfacing with \visit\ running
on the remote machine.  For the {\tt lens} machine at NCCS, the proper setup
is described below.

First, on {\tt lens}, in your {\tt .bashrc}, add:
\begin{verbatim}
export PATH="/sw/analysis-x64/visit/bin/":$PATH
\end{verbatim}
(you would think that you could just add {\tt module load visit} but this
does not seem to work with \visit.

On your local machine, launch \visit.  Note: this procedure seems to
work with \visit~2.4.2, but not \visit~2.5.0 for some reason.
\begin{itemize}
\item First we setup a new host

  \begin{itemize}
  \item From the menu, select {\em options $\rightarrow$ host profiles}
  \item Create a new host by clicking on the {\em New Host} button.
  \item Enter the {\em Host nickname} as {\tt lens}
  \item Enter the {\em Remote host name} as {\tt lens.ccs.ornl.gov}
  \item Enter the {\em Path to Visit installation} as {\tt /sw/analysis-x64/visit} (not sure if this is needed)
  \item Make sure that your {\em username} is correct
  \item Check {\em Tunnel data connections through SSH}
  \end{itemize}

\item Setup the {\em Launch Profiles}
  \begin{itemize}
  \item Click on the {\em Launch Profiles} tab
  \item Click on the {\em New Profile} button
  \item Enter the {\em Profile name} as {\tt parallel}
  \item Click on the {\em Parallel} tab
  \item Check {\em Launch parallel engine}
  \item Select the {\em Parallel launch method} as {\tt qsub/mpirun}
  \item Set the {\em Partition / Pool / Queue} to {\tt computation}
  \item Change the {\em Default number of processors} to 8
  \item Set the {\em Default number of nodes} to 2
  \item Set the {\em Default Bank / Account} to {\tt AST006}
  \item Set the {\em Default Time Limit} to {\tt 00:30:00}
  \end{itemize}

\item Click on {\em Apply} and {\em Post}

\item Save your changes by selecting {\em Options $\rightarrow$ Save Settings}
\end{itemize}

To do remote visualization, select {\em File $\rightarrow$ Open}.
From the drop down list at the top, select {\tt lens}.  You will be
prompted for your password.  After that, you can navigate to the
directory on lens with the data.

To make a movie (output sequence of images):
\begin{itemize}
\item save a view in \visit\ you like as a session file (File $\rightarrow$ Save session).  
\item On lens, create a file called {\tt files.visit} which lists all
  of the files you want to visualize, one per line, with {\tt /Header}
  after the filename.  This can be done simply as:
  \begin{verbatim}
  ls -1 | grep -v processed | awk '{print $1"/Header"}' > files.visit
  \end{verbatim}
  %$
  (note: the {\tt processed} bit is for when you used the script above to 
  automatically archive the data).

\item Edit the session file, searching for the name of the plotfile you
  originally visualized, and change it to read {\tt files.visit}.  Make
  sure that the path is correct.  This may appear multiple times.

\item Relaunch \visit\ locally and restore the session (File $\rightarrow$ Restore session).  It will render the first image.  Then reopen (File $\rightarrow$ ReOpen file).  After this is done, the buttons that allow you to move through the files should become active (black).

\item Resave the session file

\item To generate the frames, you have 2 options:

  \begin{enumerate}
  \item File $\rightarrow$ Save movie.  Pick {\em New simple movie},
    then set the format to {\em PNG} and add this to the output box by
    clicking the right arrow, then in the very last screen, select:
    {Later, tell me the command to run}.

   \visit\ will pop up a box showing the command to run.  Trying to
   get the currently running session of \visit\ to generate the frames
   seems problamatic.  Note: you will probably want to edit out the
   {\tt -v x.x.x} argument in the commandline to not have it force
   to use a specific version.

  \item If the session file successfully does the remote visualization
   as desired, you can run the movie via the commandline with something like:

   \begin{verbatim}
   visit -movie -format png -geometry 1080x1080 -output subchandra_cutoff3_ \
       -start 0 -end 44 -sessionfile subchandra_radvel.session
   \end{verbatim}

  \end{enumerate}

\end{itemize}


\section{Working at NERSC}

\subsection{edison compilers}

The default compilers on edison are the Intel compilers, but
it seems that the Cray compilers are the best.

\begin{itemize}
\item Cray 8.2.6 works well on edison/NERSC with MPI/OpenMP.

\item Cray 8.3.0 generates an internal compiler error on edison/NERSC.
      To get back to the older Cray compilers, you need to do:

\begin{verbatim}
module swap cce/8.3.0 cce/8.2.6
module swap cray-libsci cray-libsci/12.2.0
module swap cray-mpich cray-mpich/6.3.1
\end{verbatim}

\item Cray 8.3.3--8.3.7 compile, but issue a runtime error:
\begin{verbatim}
 BOXLIB ERROR: BOXARRAY_VERIFY_DIM: ba%dim not equal to some boxes dim:  3
\end{verbatim}
Again, the solution is to downgrade to the 8.2.6 version.

\end{itemize}


\subsection{General Guidelines}

edison is configured with 24 cores per node split between two Intel IvyBridge
12-core processors.  Each processor connects to 1/2 of the node's memory
and is called a NUMA node, so there are 2 NUMA nodes per edison node.
Best performance is seen when
running with 6 threads and 4 MPI tasks per node, as example:
\begin{verbatim}
aprun -n 192 -N 4 -S 2 -d 6 -ss ./main.Linux.Cray.mpi.omp.exe inputs_3d
\end{verbatim}
Here,
\begin{itemize}
\item {\tt -S} is the number of MPI tasks per NUMA node (you want to split them
  across the 2 NUMA nodes per Edison node)
\item {\tt -n} is the total number of MPI tasks for the entire application
\item {\tt -N} is the number of MPI tasks PER Edison node
\item {\tt -d} is the number of OpenMP threads per MPI task (must match 
{\tt OMP\_NUM\_THREADS})
\end{itemize}
The product of {\tt -N} and {\tt -d} should be 24---the total number of cores
per edison node.
The script {\tt
Util/job\_scripts/edison/edison.run} provides an example.

\subsubsection{Older information---still relevant?}
\noindent (2013-01-15) It's been seen that sometimes an MPICH2 error is 
generated of the form:
\begin{verbatim}
MPIU_nem_gni_get_hugepages(): Unable to mmap 8388608 bytes for file 
/var/lib/hugetlbfs/global/pagesize-2097152/hugepagefile.MPICH.1.19467.kvs_14619732, 
err Cannot allocate memory
\end{verbatim}
Setting:
\begin{verbatim}
export MPICH_GNI_MALLOC_FALLBACK=enabled
\end{verbatim}
in your submission script seems to fix this issue.  This will allow
MPI to continue if there are insufficient huge pages in one NUMA node.
(Alternately, it was suggested to run without the {\tt -ss} option to
{\tt aprun}.

\subsection{Automatic Restarting and Archiving of Data}

The same set of submission scripts described for titan are available
for edison at NERSC in {\tt Util/job\_scripts/edison/}.  They are
mostly identical, with a few minor changes for the local system
configuration.  The use is the same as described above.

\subsection{Batch visualization using \yt}

The scripts {\tt parallel-yt} and {\tt process-edison.run} in {\tt
Util/job\_scripts/edison} show how to invoke \yt\ to loop over a 
series of plotfiles and do visualization.  A number of tasks are
run at once on the node, each operating on a separate file.  Note:
it is important that {\tt aprun} be used to launch the \yt\ script
to ensure that it is run on the compute node.

\subsection{Using the {\tt AmrPostprocesing} python plotting scripts on hopper}

To build the {\tt fsnapshot.so} library, you need to do:
\begin{verbatim}
module load gcc
\end{verbatim}
{\tt f2py} is already in the path, so the library should then build without issue.
%

Then edit your {\tt .bashrc.ext} file to set the {\tt PYTHONPATH} to
the {\tt python\_plotfile} directory, e.g.:
\begin{verbatim}
export PYTHONPATH="/global/homes/z/zingale/AmrPostprocessing/python"
\end{verbatim}
%
and set the {\tt PATH} to that directory,
\begin{verbatim}
export PATH="/global/homes/z/zingale/AmrPostprocessing/python:$PATH"
\end{verbatim}

To run the script, you need to do:
\begin{verbatim}
module load matplotlib
module load python
\end{verbatim}



\subsection{Remote visualization on hopper}

\visit\ is already configured to work with hopper.  If the host does not appear
in your local version of visit, copy the {\tt host\_nersc\_hopper.xml} file
from the {\tt .visit/allhosts/} directory under the system's \visit\ install path
to your {\tt $\mathtt{\sim}$/.visit/hosts/} directory. 



\section{Working at NCSA (Blue Waters)}

\subsection{Overview}

Blue Waters consists of 22,640 Cray XE6 compute nodes and 4,224
Cray XK7 compute nodes.

Each XE node has two AMD Interlagos model 6276 compute units, each of
which has 16 integer cores (thus, a single node has a total of 32 integer
cores).  Two integer cores share a multithreaded, 256-bit wide floating 
point unit (FPU).  If both integer cores have their own thread, each has access 
to 128-bit floating point processing, whereas if only one thread is 
assigned the process can access all 256 bits.  In one major science
application on Blue Waters it was found that having an OpenMP thread for
each integer core gave the best performance, but when starting a new
application it's best to experiment.  One OpenMP thread per FPU may
be better in some cases.

Each compute unit is divided into two NUMA nodes.  Cores in
the same NUMA region share a pool of L3 cache.  For the same science
application as before it was found that the best performance was achieved
by assigning an MPI task to each NUMA node.  Thus, each physical node
has four MPI tasks.

The XK nodes consist of one AMD Interlagos model 6276 compute unit
and an NVIDIA GK110 ``Kepler'' GPU accelerator (Tesla K20X).  The
GPU is configured with 14 streaming multiprocessor units (SMXs), each
of which has 192 single-precision or 64 double-precision CUDA cores.  Thus
there are a total of 2688 SP CUDA cores or 896 DP CUDA cores.

For more details, please see 
\url{https://bluewaters.ncsa.illinois.edu/user-guide}

\subsection{BW Compilers}

The Cray compilers are the default on blue waters, and version
8.3.3 works well with \maestro.

\subsection{Monitoring Allocations}

The {\tt usage} command will list the current user's usage and 
{\tt usage -P {\em project}} will
list the usage for all users in a project allocation named ``project''.
