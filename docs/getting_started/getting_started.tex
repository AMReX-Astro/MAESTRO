

In this chapter we give an overview of \maestro, including some of the
standard problems, how to run the code, some basic runtime parameters,
and how to look at the output.

%-----------------------------------------------------------------------------
% Quick Start
%-----------------------------------------------------------------------------
\section{Quick Start}

Here we will run the standard {\tt test2} problem (three reacting
bubbles in a plane-parallel stratified atmosphere) on a single
processor.  This test problem was shown in paper~3.  

\begin{enumerate}

\item {\em Get a copy of \boxlib}.

\maestro\ requires the \boxlib\ library to manage the grids and 
parallelization.  We also rely on the build system in \boxlib\ to
build a \maestro\ executable.  \boxlib can be obtained via {\tt
git}\footnote{{\tt git} is an open-source source code version control
system.  It is a standard part of any Linux distribution.  You may
need to use your system's package manager to install {\tt git} if it
is not already present} as:
\begin{verbatim}
git clone https://ccse.lbl.gov/pub/Downloads/BoxLib.git
\end{verbatim}


\item {\em Setup your shell environment}.

\maestro\ needs to know where to find \boxlib\, by specifying the 
{\tt BOXLIB\_HOME} environment variable.  

If your shell is {\tt Bash}, add
\begin{verbatim}
export BOXLIB_HOME="/path/to/BoxLib/"
\end{verbatim}
to your {\tt .bashrc}.  

If your shell is {\tt Csh/Tcsh}, add
\begin{verbatim}
setenv BOXLIB_HOME /path/to/BoxLib/
\end{verbatim}
to your {\tt .cshrc}.  


\item {\em Setup the problem's {\tt GNUmakefile}}.

In \maestro, each problem lives under one of three sub-directories of
{\tt MAESTRO/}: {\tt SCIENCE/}, {\tt TEST\_PROBLEMS/}, or {\tt
UNIT\_TESTS/}.  This problem sub-directory will contain any
problem-specific files as well as the {\tt GNUmakefile} that specifies
how to build the executable.  Note: we rely on features of GNU {\tt
make}.  Full details of the {\tt GNUmakefile} can be found
in \S~\ref{sec:adding_problems}.  Here we will configure for a serial
build.

Change directory to {\tt MAESTRO/TEST\_PROBLEMS/test2/}.    
We only need to worry about the options at the very top of the
{\tt GNUmakefile} for now.  These should be set as follows:
\begin{itemize}
\item {\tt NDEBUG := t}

This option determines whether we compile with support for debugging
(usually also enabling some runtime checks).  Setting {\tt NDEBUG := t}
turns off debugging.

\item {\tt MPI := }

This determines whether we are doing a parallel build, using the Message
Passing Interface (MPI) library.  For now, we leave this option empty,
disabling MPI.  This will build \maestro\ in serial mode, so no MPI
library needs to be present on the host system.

\item {\tt OMP := }

This determines whether we are using OpenMP to do parallelism within a
shared memory node.  OpenMP is used together with MPI, with MPI
distributing the grids across the processors and within a
shared-memory node, OpenMP allows many cores to operate on the same
grid.  For now, we leave this option empty, disabling OpenMP.

\item {\tt SDC := }

This option determines whether we want to use the alternate SDC
(Spectral Deferred Corrections) source for \maestro\ (see
Chapter \ref{ch:sdc}).  This is experimental, and this option not present
in all problems.  We leave this blank so we compile the
default \maestro\ source.

\item {\tt COMP := gfortran}

This option specifies the Fortran compiler.  We will use {\tt gfortran}, which is
the preferred compiler for \maestro.  Specifying this compiler will automatically
pull in the compiler settings as specified in {\tt BOXLIB\_HOME/Tools/F\_mk/}.

\end{itemize}


\item {\em Build the executable}.

Type {\tt make}.  The build system will first find the dependencies
amongst all the source files and then build the executable.  When
finished, the executable will have a name like {\tt
main.Linux.gfortran.exe}, where the specific parts of the name depend
on the compilers and OS used.


\item {\em Copy in auxillary files}.

This problem uses the {\tt helmeos} equation of state, so 
we need to copy in the data table for the equation of state, {\tt
  extern/EOS/helmeos/helm\_table.dat}.

\begin{verbatim}
cp ../../../extern/EOS/helmeos/helm_table.dat .
\end{verbatim}


\item {\em Run!}

Each problem requires an input file.  The inputs file consists of
lines of the form {\em parameter = value}, where {\em parameter} is
one of the many runtime parameters \maestro\ knows, and {\em value}
overrides the default value for that parameter.  For the {\tt test2}
problem, we will use the inputs file {\tt inputs\_2d}.  An overview of
some of the more common runtime parameters is given
in \S~\ref{sec:gettingstarted:runtime}, and a full list of
all \maestro\ runtime parameters and their default values is given in
Chapter~\ref{ch:runtimeparameters}.

\maestro\ is run simply as:
\begin{verbatim}
 ./main.Linux.Intel.exe inputs_2d
\end{verbatim}
We can also override the default value of any runtime parameter by specifying
it on the commandline as
\begin{verbatim}
 ./main.Linux.Intel.exe inputs_2d --parameter value
\end{verbatim}

As the code runs, a lot of information will pass through the screen.
For each timestep, each of the steps 1 through 12 shown in
the \maestro\ flowchart (Chapter~\ref{ch:flowchart}) will be shown
along with diagnostic information about the solution.  Upon completion
some memory usage information is printed.


\item {\em Examine the output}.

As the code runs, it will output both plotfiles and checkpoints as
well as one or more text diagnostic files ({\tt maestro\_diag.out} by
default) with integral or extrema information (like maximum Mach
number) from each timestep.

By default, the plotfiles will be named {\tt plt}{\em nnnnn}, where
the number {\em nnnnn} is the timestep number when the file was
outputted.  Similarly, the checkpoints are named {\tt chk}{\em
nnnnn}.  \boxlib\ plotfiles and checkpoints are actually directories,
with the data stored in sub-directories grouped by refinement level.
Details of the simulation (build information, number of processors
used, output date, output directory, runtime parameter values, ...)
are stored in the {\tt job\_info} file in each plotfile directory.

Visualization of results is described in the next section.


\end{enumerate}


\section{Working with the Output}

Visualization and analysis are done on the plotfiles.  A
number of in-house and externally developed tools can work 
with \boxlib-formatted plotfiles.


\subsection{\amrvis}

\amrvis\ is an easy-to-use visualization tool developed at LBL for
2- and 3D datasets which can plot slices through 3D datasets as well
as volume-renderings.  It can also very easily extract 1D lines
through the dataset along any coordinate direction.  It is distributed
separately from the \maestro\ distribution.

\amrvis\ can be obtained via git as:
\begin{verbatim}
git clone https://ccse.lbl.gov/pub/Downloads/Amrvis.git
\end{verbatim}
\amrvis\ is built in the C++ \boxlib\ framework (instead of the Fortran 
\boxlib\ framework that \maestro\ uses).  The build systems are similar,
but differ in a few ways.  

\amrvis\ uses the {\tt Motif} library for defining the GUI.  On a Linux 
system, you may need to install the {\tt lesstif} package and any
related development packages (e.g.\ {\tt lesstif-devel}).  Depending
on your Linux system, you may also need to install {\tt libXpm} 
and related development packages (e.g.\ {\tt libXpm-devel}).

%need to modify _f95_lib 

library line


\subsection{{\tt data\_processing} scripts}

Several useful analysis scripts (written in Fortran 90) can be found
in {\tt fParallel/data\_processing}.  The {\tt GNUmakefile} there
needs to be edited to indicate which of the tools to build.  For
example, to extract the density along a line from the center of a
plotfile, {\tt plt00200}, in the $y$-direction:

\begin{verbatim}
fextract.Linux.gfortran.exe -d 2 -v "density" -p plt00200
\end{verbatim}

These routines are described in \S~\ref{sec:analysis}.

There is also a python visualization method in {\tt
fParallel/data\_processing/python\_plotfile}.  This is described
in \S~\ref{sec:vis:python}.


\subsection{\visit}

\visit\ is a powerful, DOE-supported visualization tool for 2- and 3D
datasets.  It can do contouring, volume rendering, streamlines, ...\, ,
directly from \boxlib\ plotfiles (to open a plotfile in \visit, point to
the {\tt Header} file inside the plotfile directory).  Details on
\visit\ can be found at:\newline
 {\tt https://wci.llnl.gov/codes/visit/home.html}\,. \newline
The easiest way to get started with \visit\ is to download a precompiled
binary from the \visit\ webpage.

Once \visit\ is installed, you can open a \boxlib\ plotfile by pointing
\visit\ to the {\tt Header} file in the plotfile directory.

\subsection{Diagnostic Files}

By default, \maestro\ outputs global diagnostics each timestep into a file
called {\tt maestro\_diag.out}.  This includes the maximum Mach number,
peak temperature, and peak nuclear energy generation rate.  Individual problems
can provide their own {\tt diag.f90} file to produce custom diagnostic
output.





\section{`Standard' Test Problems}

Different problems in \maestro\ are contained in one of three
sub-directories under {\tt MAESTRO/} ({\tt SCIENCE/}, {\tt
TEST\_PROBLEMS}, or {\tt UNIT\_TESTS}).  The {\tt GNUmakefile} in each
problem directory lists the components of {\tt MAESTRO} that are used
to build the executable.  {\tt TEST\_PROBLEMS/} contains simple
problems that were used in the development of \maestro.  Many
of these were featured in the papers describing the \maestro\ algorithm.

Some of the test problems available are:
\begin{itemize}
\item {\tt flame} \\[-3mm]

The {\tt flame} problem models a planar laminar flame.  Initially cool
fuel and hot ash are put in pressure equilibrium.  Thermal diffusion
transfers heat to the fuel, driving a burning front.  This problem
does not use the traditional \maestro\ elliptic constraint, but
rather enforces $\nabla \cdot U = S$ (through the runtime parameter
{\tt do\_smallscale = T}) as discussed in \cite{SNe}.

\item {\tt spherical\_heat} \\[-3mm]

{\tt spherical\_heat} maps a spherical star (a Chandrasekhar-mass white
dwarf) onto the Cartesian domain and uses an external heat source to
cause the star to expand.  This is the 3D analogue of the {\tt
  test\_basestate} unit test (see \S~\ref{sec:unit_tests}).  This
problem was discussed in \cite{multilevel}.

\item {\tt test2} \\[-3mm]

{\tt test2} places 3 hots spots in a plane-parallel atmosphere.
Burning makes these bubbles buoyant, and then roll up.  This problem was
used in \cite{lowMach3} to compare with compressible solvers.

\item {\tt test\_convect} \\[-3mm]

{\tt test\_convect} drives convection through a plane-parallel
atmosphere using an externally-specified heat source.  This problem
was used to compare with compressible solvers in \cite{lowMach3}
and to test the multilevel algorithm in \cite{multilevel}.

\item {\tt test\_spherical} \\[-3mm]

{\tt test\_spherical} sets up an isentropically stratified star
and stirs it up with a random velocity field.  The low Mach number
constraint is replaced with the anelastic constraint (through
the {\tt beta\_type} runtime parameter).  Analytically, under
these conditions, the density of the star should not change.
This test problem was discussed in \cite{lowMach4}.

\end{itemize}




\section{Common Runtime Parameters}
\label{sec:gettingstarted:runtime}

\subsection{Controlling Timestepping and Output}

Parameters that set the maximum time for the simulation to run
include:
\begin{itemize}
\item {\tt stop\_time} is the maximum simulation time, in seconds,
      to evolve the system for.

\item {\tt max\_step} is the maximum number of steps to take.
\end{itemize}

\noindent Parameters affecting the size of the timestep include:
\begin{itemize}
\item {\tt cflfac} is a multiplicative factor ({\tt $\le 1$}) 
      applied to the advective CFL timestep

\item {\tt init\_shrink} is the factor ({\tt $\le 1$}) by which to reduce 
      the initial timestep from the estimated first timestep.
\end{itemize}

\noindent Parameters affecting output and restart include:
\begin{itemize}

\item {\tt restart} tells \maestro\ to restart from a checkpoint.  The
      value of this parameter should be the file number to restart from.
      For example, to restart from the checkpoint file {\tt chk00010},
      you would set {\tt restart = 10}.

\item {\tt plot\_int} is the number of steps to take between
  outputting a plotfile

\item {\tt plot\_deltat} is the simulation time to evolve between
  outputting a plotfile.  Note: to output only based on simulation
  time, set {\tt plot\_int = -1}.

\item {\tt check\_int} is the number of steps to take between
  outputting a checkpoint.

\end{itemize}

\subsection{Defining the Grid and Boundary Conditions}

Parameters that determine the spatial extent of the grid, 
the types of boundaries, and the number of computational cells include:
\begin{itemize}

\item {\tt max\_levs } is the maximum number of grid levels in the AMR
  hierarchy to use.  {\tt max\_levs = 1} indicates running with only a
  single level spanning the whole domain.

\item {\tt n\_cellx }, {\tt n\_celly }, {\tt n\_cellz } the size of
  base level in terms of number of cells, in the $x$, $y$, and $z$
  coordinate directions.

\item {\tt max\_grid\_size } the maximum extend of a grid, in any
  coordinate direction, as measured in terms of number of cells.

  For multilevel problems, the parameter {\tt max\_grid\_size\_1}
  controls the maximum extent on level 1 (the base grid), {\tt
    max\_grid\_size\_2} controls the maximum extent on level 2, and
  {\tt max\_grid\_size\_3} controls the maximum extent on levels 3 and
  higher.

\item {\tt prob\_lo\_x }, {\tt prob\_lo\_y }, {\tt prob\_lo\_z } is
  the physical coordinate of the lower extent of the domain boundary
  in the $x$, $y$, and $z$ coordinate directions.

\item {\tt prob\_hi\_x }, {\tt prob\_hi\_y }, {\tt prob\_hi\_z } is
  the physical coordinate of the upper extent of the domain boundary
  in the $x$, $y$, and $z$ coordinate directions.

\item {\tt bcx\_lo }, {\tt bcy\_lo }, {\tt bcz\_lo }, 
      {\tt bcx\_hi }, {\tt bcy\_hi }, {\tt bcz\_hi } are the boundary
   condition types at the lower (`{\tt lo}') and upper (`{\tt hi}')
   domain boundaries in the $x$, $y$, and $z$ coordinate directions.
   The different types are set via integer flags listed in table~\ref{arch:table:bcflags}.

   \begin{table}[h]
   \caption{\label{arch:table:bcflags} Boundary condition flags}  
   \begin{center}
   \begin{tabular}{ll}
   \hline
   BC type    & integer flag \\
   \hline
   periodic             & $-1$ \\
   inlet (user-defined) & $11$ \\
   outlet               & $12$ \\
   symmetry             & $13$ \\
   slip wall            & $14$ \\
   no-slip wall         & $15$ \\
   \hline
   \end{tabular}
   \end{center}
   \end{table}

\end{itemize}

Note that grid cells must be square, i.e. $\Delta x = \Delta y = \Delta z$
where $\Delta x$ on the base grid is computed as $({\tt prob\_hi\_x}
- {\tt prob\_lo\_x})/{\tt n\_cellx}$.  For multilevel problems, the effective
number of zones on the finest grid in the $x$ direction will be
${\tt n\_cellx} \cdot 2^{({\tt max\_levels} -1)}$.



